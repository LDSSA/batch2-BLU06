{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c583549321a1c379f8870931303d3ecc",
     "grade": false,
     "grade_id": "cell-b88a7e663ede9739",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# BLU06 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "72b372981b2e4d24d8b3632596a77045",
     "grade": false,
     "grade_id": "cell-4f0cbbbc1eadab9f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "67ff836e96623f15b5d22db83daf510b",
     "grade": false,
     "grade_id": "cell-337eee9fcbab05b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1 Read the Programs data (graded)\n",
    "\n",
    "In this first exercise, we aim to create a single data frame, combining all programs from all seasons.\n",
    "\n",
    "With a caveat though: **we want to include seasons after 1900**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fffb0e56526ec2d940e9cfe370047fde",
     "grade": false,
     "grade_id": "cell-bf4694e22046237e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_programs():\n",
    "    files = os.listdir('data/programs/')\n",
    "    # Create a list with the name of all files containing programs from\n",
    "    # 1900 inclusive and onwards (just the filename, no complete path.)\n",
    "    # files_after_1900: List[str] = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Create a list with the name of all .csv files.\n",
    "    # seasons: List[pd.DataFrame] = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Use pd.concat to create a single dataframe.\n",
    "    # programs: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Drop the column ProgramID.\n",
    "    # programs = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Set the index to be the column GUID, according best practices.\n",
    "    # Feel free to use method chaining if you want.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return programs\n",
    "\n",
    "\n",
    "def read_season(file):\n",
    "    path = os.path.join('data', 'programs', file)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "programs = make_programs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d5bb811f1664be588c2bf34116a3a25b",
     "grade": true,
     "grade_id": "cell-1aa243b8e5df1a17",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(programs['Season'].min() == '1900-01')\n",
    "\n",
    "shape = str(programs.shape)\n",
    "expected_hash = 'd4e990f328604be98d2ce91ec7cead4660b79c41da0fffa174bd83fc43928c77'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(programs.sample(random_state=0))\n",
    "expected_hash = '2ed2dbfa0da6dd4ea08fd025b5b744a716c8b47a7a23a06b0cff6198c8e69e94'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5db4dd97b8ad7802781579d1211daafc",
     "grade": false,
     "grade_id": "cell-673495903dcba4a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's preview the `programs` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "60987a73866ff9538425a3f4469ba8ec",
     "grade": false,
     "grade_id": "cell-fc75b7c4c275a431",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "programs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f27bcfdf1812642a9e2251be0b2e826",
     "grade": false,
     "grade_id": "cell-0317b5a17a59d85f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2 Read the Concerts data (graded)\n",
    "\n",
    "Read the concerts data.\n",
    "\n",
    "Although we list all transformations step-by-step for the sake of clarity, we expect you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5ad5b6e3e0c27b5e8b09c524205817f3",
     "grade": false,
     "grade_id": "cell-58bd5fde4e09e377",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_concerts(): \n",
    "    # Read concerts data and drop the ProgramID and ConcertID columns.\n",
    "    # concerts: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Remember to_datetime? We need here. We need to parse the columns Date and \n",
    "    # Time. Use pd.to_datetime(...).dt.date for the Date and pd_to_datetime(..., \n",
    "    # format=%I:%M%p).dt.time for the Time.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return concerts\n",
    "\n",
    "\n",
    "concerts = make_concerts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "510ea1201ee244c9bb32dc62828c1ea2",
     "grade": true,
     "grade_id": "cell-1e95492b37c4c43f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(concerts.shape)\n",
    "expected_hash = 'c030586e7370b1f2c34307d5de9b921d96efa28c933e44111b121ed819f339da'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(concerts.sample(random_state=0))\n",
    "expected_hash = '392a3db01753b02d85173c38cde95112fb5cdf06ca5a45d25f828238d56103be'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c9fcbbdc690a2f50662ed9f5ac4515d4",
     "grade": false,
     "grade_id": "cell-e22fb5671017b5bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "concerts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5ae63080e844bdc98785e34916df0e71",
     "grade": false,
     "grade_id": "cell-f0653d1fbb5f9043",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3 Combine Programs and Concerts data (graded)\n",
    "\n",
    "Let's combine both data frames into a single dataset, using an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "68e80c6593f503980b37abef95ed7c6f",
     "grade": false,
     "grade_id": "cell-110744d4ad3ef2ef",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Remember that you want to join on the index of one of the dataframes.\n",
    "# nyp = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "128bd45dcf5ad3c5ccf5cecbf7dee578",
     "grade": true,
     "grade_id": "cell-b7730d3cb9832724",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp.shape)\n",
    "expected_hash = 'ed5f695b3d2a0e790163dc73f7fe7e1dc56cc53c6f13289a198702ed4094071c'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(nyp.sample(random_state=0))\n",
    "expected_hash = '3fa1f19dfd4c9d82b76c963890951b75e1c56a464bc0e592f5f646068ad382d4'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7f75efbc3c1f26d0d8169490d9a6ac7a",
     "grade": false,
     "grade_id": "cell-ac54633ced3ec3ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4 Read Works and Soloists data (graded)\n",
    "\n",
    "We will read the two remaining pieces of data. \n",
    "\n",
    "Again, albeit the step-by-step description, we encourage you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "acb915c94d88dfaf152ca1bdeba71a71",
     "grade": false,
     "grade_id": "cell-030bd11a9b6c76c8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_works():\n",
    "    # Read the works data.\n",
    "    # works: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Remove the intervals.\n",
    "    # works: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Select the columns GUID, ComposerName, WorkTitle, Movement and ConductorName.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return works\n",
    "\n",
    "\n",
    "def make_soloists():\n",
    "    # Read the soloists data and drop ProgramID, WorkID and MovementID.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return soloists\n",
    "\n",
    "\n",
    "works = make_works()\n",
    "soloists = make_soloists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1441666c745b91a9ed9302e2720234ff",
     "grade": true,
     "grade_id": "cell-2a37eb128ea7e979",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(works.shape)\n",
    "expected_hash = 'cad58aa6cd33cfa24c08a0f0f846877178ab31278f212c80b16b952d9416f883'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(works.sample(random_state=0))\n",
    "expected_hash = 'a27f5e63c23dcbe3db496bf7e1d3f7bb3ae9d3cd2cd1ff1d0498c73c66bd5703'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "shape = str(soloists.shape)\n",
    "expected_hash = 'a7b0d20a45ff1344e0398eebb162af9afb8805082b0dfdcb70e9a4b78f94dd13'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(soloists.sample(random_state=0))\n",
    "expected_hash = '3352eb27df2a6dba982be1001af1af39a3adc031134f44fe14c90f34cdd1493e'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6219f402c784dc4cd4679e5c099c5228",
     "grade": false,
     "grade_id": "cell-fb54d5d81dda97e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 5 Combine Works and Soloists (graded)\n",
    "\n",
    "Like we did for Programs and Concerts, now we combine Works and Soloists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0b9b38d0ff8d0c603bcc617ee94583be",
     "grade": false,
     "grade_id": "cell-3c2e4becb723a4a3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine both dataframes, again using an inner type of join.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "894b6966c3a31ce8ddc5b44be1c96210",
     "grade": true,
     "grade_id": "cell-13616b91f6c53cad",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(works_and_soloists.shape)\n",
    "expected_hash = 'c0e73877aac4f3916267cb58f2f122ffef32c79039bde2ecb217fda123270d12'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(works_and_soloists.sample(random_state=0))\n",
    "expected_hash = 'e604d9c53131c6aabe798ef85e07413fbdd9090cc7e2d4fe911e6e23a0b5e7a8'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b7ee54996423cde626ca1b37a96645d",
     "grade": false,
     "grade_id": "cell-463cdbc4a6b9ab87",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 6 Combine everything (graded)\n",
    "\n",
    "The final goal here is to create a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "17510c2e39228be2f14d8ecdfe97a563",
     "grade": false,
     "grade_id": "cell-081458c0c0a40de2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine everything into a single dataframe.\n",
    "# nyp = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "314632c24350957d11af94a32414b8ce",
     "grade": true,
     "grade_id": "cell-a0ca421b4504282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp.shape)\n",
    "expected_hash = '9f690a900c6373cf2cb0ccf8748b0e63fdbf9ca201542794e3ef952cccdb4da3'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(nyp.sample(random_state=0))\n",
    "expected_hash = '5e0a8c4e71b3a0ecb7c81b6719a2f9d9a4d1adc2ca34cd763a758ab6a1867c4c'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0178d74e5fa64f0f9e9e1bb228168580",
     "grade": false,
     "grade_id": "cell-76e3b2877f0a18aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 7 Final transformations (graded)\n",
    "\n",
    "Now, we perform the train-test split.\n",
    "\n",
    "We also perform some final transformations on both datasets:\n",
    "* Include some date features: Year, Month, Day and Weekday\n",
    "* Drop Date, Season and GUID\n",
    "* Change the column name Orchestra to OrchestraName, for consistency with other name columns\n",
    "* Filter out composers that appear in less than 100 concerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fdba9c2b3450bcbb1a66df6d663b8d7b",
     "grade": false,
     "grade_id": "cell-f8ecf5eba7d6c38b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # You should follow these exact steps:\n",
    "    #   1 - add_date_features, ideally using df.pipe\n",
    "    #   2 - drop Date, Season and GUID\n",
    "    #   3 - rename Orchestra to OrchestraName\n",
    "    #   4 - filter out composers with less that 100 concerts (>= 100 rows)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df\n",
    "\n",
    "\n",
    "nyp_ = preprocess_data(nyp)\n",
    "X_train, X_test = train_test_split(nyp_, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0c0a445641d6219f7870f67a05c2608c",
     "grade": true,
     "grade_id": "cell-77f754825659deb6",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp.shape)\n",
    "expected_hash = '9f690a900c6373cf2cb0ccf8748b0e63fdbf9ca201542794e3ef952cccdb4da3'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(nyp.sample(random_state=0))\n",
    "expected_hash = '5e0a8c4e71b3a0ecb7c81b6719a2f9d9a4d1adc2ca34cd763a758ab6a1867c4c'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "shape = str(nyp_.shape)\n",
    "expected_hash = '0d9fc79d1c283ef411b3d7a0d94d2c9077526ce2ca95526a55c3eb44fd321b3d'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(nyp_.sample(random_state=0))\n",
    "expected_hash = '6fb60e43e3a7b600032305c93743963e6e2578cc8b1d6afd11b80fa9ac2daa57'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "columns = str(nyp_.columns.values)\n",
    "expected_hash = '7d131b98b4d7094443c094603c6db00aa20a79e49661acdefb33bf5fc1c071fa'\n",
    "assert(hashlib.sha256(columns.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aacb76e1129c59e9c31d1f25dc620740",
     "grade": false,
     "grade_id": "cell-6daa8cde1618bc1f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "And, finally, we would be ready to explore modeling.\n",
    "\n",
    "For the next part, however, we will be using the famous [Boston House Prices Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7593ad4c522efea4bada2529ac897bb1",
     "grade": false,
     "grade_id": "cell-d7cd808ac07305b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 8 Scaling features (graded)\n",
    "\n",
    "About the Boston dataset:\n",
    "\n",
    "> Each record in the database describes a Boston suburb or town. The data is from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970.\n",
    "\n",
    "The features are all numerical (real, positive):\n",
    "* **CRIM** - per capita crime rate by town\n",
    "* **ZN** - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* **INDUS** - proportion of non-retail business acres per town\n",
    "* **CHAS** - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* **NOX** - nitric oxides concentration (parts per 10 million)\n",
    "* **RM** - average number of rooms per dwelling\n",
    "* **AGE** - proportion of owner-occupied units built prior to 1940\n",
    "* **DIS** - weighted distances to five Boston employment centres\n",
    "* **RAD** - index of accessibility to radial highways\n",
    "* **TAX** - full-value property-tax rate per \\$10,000\n",
    "* **PTRATIO** - pupil-teacher ratio by town\n",
    "* **B** - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* **LSTAT** - % lower status of the population\n",
    "* **MEDV** - Median value of owner-occupied homes in \\$1000's.\n",
    "\n",
    "We want to scale all features to the same range, using `sklearn.preprocessing.MinMaxScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f371435865c1dafaa925ac0642c203cb",
     "grade": false,
     "grade_id": "cell-520f1d8e99c15a49",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Initialize the MinMaxScaler to a [0, 5] range.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Fit on the training set and transform X_train. We expect X_train_\n",
    "# to be a dataframe **just like** X_train, only scaled. \n",
    "# X_train_: pd.DataFrame = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Transform the test set.\n",
    "# X_test_: pd.DataFrame = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1f6118a4a2b9973e8d956f4536393a3e",
     "grade": true,
     "grade_id": "cell-39a52095c35ce047",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(X_train_.shape)\n",
    "expected_hash = '6f696c7e30c15aae3f0fa4807b596cf15d28cadaf33602d8d20368f7ac921f26'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(X_train_.sample(random_state=0))\n",
    "expected_hash = '158be9e6cfe3865a8ea5baffb8b8cc231030aa07bc114539de0df244e91ac5ec'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)\n",
    "_\n",
    "columns = str(X_train_.columns.values)\n",
    "expected_hash = 'c4e20218e7e33f0e771a608bb05ece0152f5a15fc6a0629b6c88cef7790fbfe1'\n",
    "assert(hashlib.sha256(columns.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "shape = str(X_test_.shape)\n",
    "expected_hash = 'aa2b4e3c1e358b4b9f21c2c86bbf1187020582395419f1a02a949d7a6efac9e4'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(X_test_.sample(random_state=0))\n",
    "expected_hash = 'e2ea8d5b9b195cc081124efca8af59372b4004364afa3f78b5a50ea13149f9bb'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "columns = str(X_test_.columns.values)\n",
    "expected_hash = 'c4e20218e7e33f0e771a608bb05ece0152f5a15fc6a0629b6c88cef7790fbfe1'\n",
    "assert(hashlib.sha256(columns.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e4d42e44ae4c7cf2210da2cda97ac6c",
     "grade": false,
     "grade_id": "cell-458080a73da057e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 9 Build a ColumnSelector transformer (graded)\n",
    "\n",
    "There's a simple transformer that can be useful, from times to times, when modeling.\n",
    "\n",
    "What we want is to build a transformer that returns the columns we select beforehand. \n",
    "\n",
    "This transformer could be used to determine what features go into modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e5ecf45eff13a2a61cfcfcefa55dd85e",
     "grade": false,
     "grade_id": "cell-3d8545164af32b84",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    # Implement the __init__ method.\n",
    "    # Our ColumnSelector must be able to receive a parameter columns.\n",
    "    # The default value for columns must be set to 'all', so we can\n",
    "    # initialize it without any explicit parameters.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "    # There's no need for a fit method in this case, it does nothing.\n",
    "    # We should be able to call fit without any explicit parameters.\n",
    "    # Meaning: we should be able to call ColumnSelector.fit().\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Transform should return all columns if the parameter columns we\n",
    "    # passed upon initialization is equal to 'all'. If a column or a\n",
    "    # list of columns are passed, only those should be returned.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "\n",
    "cols = ['CRIM', 'DIS', 'INDUS', 'RM', 'DIS', 'TAX', 'B']\n",
    "selector = ColumnSelector(columns=cols)\n",
    "X_train__ = selector.fit_transform(X_train_)\n",
    "X_test__ = selector.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "616699138c6038978e2a9f0a5f056986",
     "grade": true,
     "grade_id": "cell-6d54a8e7ed69bd97",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(ColumnSelector())\n",
    "assert(selector.fit())\n",
    "\n",
    "shape = str(X_train__.shape)\n",
    "expected_hash = '5d4f688e84beb21ec07f136c16a6cc11318d4f5de7b81bf0232e5282d9834123'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(X_train__.sample(random_state=0))\n",
    "expected_hash = '410bd230a9e60a209d5f2a9f705e41534149d7de98a0c0a4607d9c4121250437'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "columns = str(X_train__.columns.values)\n",
    "expected_hash = '901009bce1feeeccadd8cd499664598ff9319641e55dcda17a650c13c0626604'\n",
    "assert(hashlib.sha256(columns.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "shape = str(X_test__.shape)\n",
    "expected_hash = '0aba1c19151f76aa2ecb00fd75be05c6f73860573972e967f3d1fe1c44ae2629'\n",
    "assert(hashlib.sha256(shape.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(X_test__.sample(random_state=0))\n",
    "expected_hash = '38a69d5837865bb4d5d6f6008331b4ae3449cee61aa10609b7db5fa73029c518'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "columns = str(X_test__.columns.values)\n",
    "expected_hash = '901009bce1feeeccadd8cd499664598ff9319641e55dcda17a650c13c0626604'\n",
    "assert(hashlib.sha256(columns.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c0965502388b047e7b1e6c4f292fd90",
     "grade": false,
     "grade_id": "cell-f751d80b4180bcd0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 10 Building the pipeline (graded)\n",
    "\n",
    "Finally, we want to use the two transformers together and run a linear regression on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "020249acd83b4ee429dc517f653116ad",
     "grade": false,
     "grade_id": "cell-c6cf45c70a05aa5c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline including:\n",
    "#   1 - 'selector', ColumSelector(columns=cols)\n",
    "#   2 - 'min_max', MinMaxScaler() with same range as above\n",
    "#   3 - 'model', LinearRegression\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('MSE: {}'.format(mse))\n",
    "print('MAE: {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "22fdb6b5cebb41c8b7afb472bb129168",
     "grade": true,
     "grade_id": "cell-b520a77a388d20b2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(type(pipeline) == Pipeline)\n",
    "\n",
    "params = str(pipeline.get_params)\n",
    "expected_hash = '06f6c462bed73d9ff134dbfd1a8081c2c9f7e5e61e8166b5e14f4a3cf9af63ff'\n",
    "assert(hashlib.sha256(params.encode()).hexdigest() == expected_hash)\n",
    "\n",
    "sample = str(pd.Series(y_pred).sample(random_state=0))\n",
    "expected_hash = 'f3ce987cb9aa2ef4caf2b86400515836c2ec8e2b620fbd94170f449f885f0195'\n",
    "assert(hashlib.sha256(sample.encode()).hexdigest() == expected_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9982176fa793c9578e9d5eee90fd30f4",
     "grade": false,
     "grade_id": "cell-877f9bbc108ee8be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Exercises complete, congratulations! You are about to become a certified data wrangler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
